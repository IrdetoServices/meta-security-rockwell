From 3833e699506ff7c3121f38c333a1fa7521ad69b1 Mon Sep 17 00:00:00 2001
From: Ben Gardiner <ben.gardiner@irdeto.com>
Date: Tue, 27 Oct 2015 20:11:07 -0400
Subject: [PATCH] MPROTECT patch extracted from grsecurity

---
 arch/arm/mm/fault.c        |  36 +++++
 fs/binfmt_aout.c           |  18 +++
 fs/binfmt_elf.c            | 317 +++++++++++++++++++++++++++++++++++++++++++++
 fs/exec.c                  | 171 ++++++++++++++++++++++++
 fs/proc/array.c            |  23 ++++
 grsecurity/Kconfig         |   5 +
 include/kh/kh_common.h     |  32 +++++
 include/linux/binfmts.h    |   4 +
 include/linux/mm.h         |   8 ++
 include/linux/mm_types.h   |   4 +
 include/linux/sched.h      |  42 ++++++
 include/uapi/linux/a.out.h |  10 ++
 include/uapi/linux/elf.h   |  32 +++++
 ipc/shm.c                  |   9 ++
 mm/mmap.c                  |  99 +++++++++++++-
 mm/mprotect.c              |  23 +++-
 security/Kconfig           |   1 +
 17 files changed, 832 insertions(+), 2 deletions(-)
 create mode 100644 grsecurity/Kconfig
 create mode 100644 include/kh/kh_common.h

diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index eb8830a..1a5a321 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -26,6 +26,8 @@
 #include <asm/system_info.h>
 #include <asm/tlbflush.h>
 
+#include <kh/kh_common.h>
+
 #include "fault.h"
 
 #ifdef CONFIG_MMU
@@ -174,6 +176,13 @@ __do_user_fault(struct task_struct *tsk, unsigned long addr,
 	}
 #endif
 
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"Page execution fault reporting")
+	if (fsr & FSR_LNX_PF) {
+		pax_report_fault(regs, (void *)regs->ARM_pc, (void *)regs->ARM_sp);
+		do_group_exit(SIGKILL);
+	}
+	KH_FEATURE_END(PAX_PAGEEXEC)
+
 	tsk->thread.address = addr;
 	tsk->thread.error_code = fsr;
 	tsk->thread.trap_no = 14;
@@ -401,6 +410,33 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 }
 #endif					/* CONFIG_MMU */
 
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"pax_report_insns implementation (arch dependent fault reporting")
+void pax_report_insns(struct pt_regs *regs, void *pc, void *sp)
+{
+	long i;
+
+	printk(KERN_ERR "PAX: bytes at PC: ");
+	for (i = 0; i < 20; i++) {
+		unsigned char c;
+		if (get_user(c, (__force unsigned char __user *)pc+i))
+			printk(KERN_CONT "?? ");
+		else
+			printk(KERN_CONT "%02x ", c);
+	}
+	printk("\n");
+
+	printk(KERN_ERR "PAX: bytes at SP-4: ");
+	for (i = -1; i < 20; i++) {
+		unsigned long c;
+		if (get_user(c, (__force unsigned long __user *)sp+i))
+			printk(KERN_CONT "???????? ");
+		else
+			printk(KERN_CONT "%08lx ", c);
+	}
+	printk("\n");
+}
+KH_FEATURE_END(PAX_PAGEEXEC)
+
 /*
  * First Level Translation Fault Handler
  *
diff --git a/fs/binfmt_aout.c b/fs/binfmt_aout.c
index 929dec0..f759f8c 100644
--- a/fs/binfmt_aout.c
+++ b/fs/binfmt_aout.c
@@ -30,6 +30,8 @@
 #include <asm/cacheflush.h>
 #include <asm/a.out-core.h>
 
+#include <kh/kh_common.h>
+
 static int load_aout_binary(struct linux_binprm *);
 static int load_aout_library(struct file*);
 
@@ -261,6 +263,22 @@ static int load_aout_binary(struct linux_binprm * bprm)
 
 	install_exec_creds(bprm);
 
+	KH_FEATURE_BEGIN(PAX_NOEXEC,"Initialize mm_struct pax_flags")
+	current->mm->pax_flags = 0UL;
+	KH_FEATURE_END(PAX_NOEXEC)
+
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+	if (!(N_FLAGS(ex) & F_PAX_PAGEEXEC)) {
+		current->mm->pax_flags |= MF_PAX_PAGEEXEC;
+
+	KH_FEATURE_BEGIN(PAX_MPROTECT,"Imbricated")
+		if (!(N_FLAGS(ex) & F_PAX_MPROTECT))
+			current->mm->pax_flags |= MF_PAX_MPROTECT;
+	KH_FEATURE_END(PAX_MPROTECT)
+
+	}
+	KH_FEATURE_END(PAX_PAGEEXEC)
+
 	if (N_MAGIC(ex) == OMAGIC) {
 		unsigned long text_addr, map_size;
 		loff_t pos;
diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
index 3dd2497..88eee0f 100644
--- a/fs/binfmt_elf.c
+++ b/fs/binfmt_elf.c
@@ -38,6 +38,8 @@
 #include <asm/param.h>
 #include <asm/page.h>
 
+#include <kh/kh_common.h>
+
 #ifndef user_long_t
 #define user_long_t long
 #endif
@@ -65,6 +67,10 @@ static int elf_core_dump(struct coredump_params *cprm);
 #define elf_core_dump	NULL
 #endif
 
+KH_FEATURE_BEGIN(PAX_MPROTECT,"elf_handle_mprotect declaration")
+static void elf_handle_mprotect(struct vm_area_struct *vma, unsigned long newflags);
+KH_FEATURE_END(PAX_MPROTECT)
+
 #if ELF_EXEC_PAGESIZE > PAGE_SIZE
 #define ELF_MIN_ALIGN	ELF_EXEC_PAGESIZE
 #else
@@ -84,6 +90,12 @@ static struct linux_binfmt elf_format = {
 	.load_binary	= load_elf_binary,
 	.load_shlib	= load_elf_library,
 	.core_dump	= elf_core_dump,
+
+KH_FEATURE_BEGIN(PAX_MPROTECT,"elf_format struct handle_mprotect initialization")
+	.handle_mprotect= elf_handle_mprotect,
+	.handle_mmap	= NULL,
+KH_FEATURE_END(PAX_MPROTECT)
+
 	.min_coredump	= ELF_EXEC_PAGESIZE,
 };
 
@@ -543,6 +555,129 @@ out:
 	return error;
 }
 
+
+KH_FEATURE_BEGIN(PAX_NOEXEC,"PAX MPROTECT ELF Configuration")
+
+KH_FEATURE_BEGIN(CONFIG_PAX_PT_PAX_FLAGS,"We implement only hardmode")
+static unsigned long pax_parse_pt_pax_hardmode(const struct elf_phdr * const elf_phdata)
+{
+	unsigned long pax_flags = 0UL;
+
+	if (!(elf_phdata->p_flags & PF_NOPAGEEXEC))
+		pax_flags |= MF_PAX_PAGEEXEC;
+
+	if (!(elf_phdata->p_flags & PF_NOSEGMEXEC))
+		pax_flags |= MF_PAX_SEGMEXEC;
+
+	if (!(elf_phdata->p_flags & PF_NOMPROTECT))
+		pax_flags |= MF_PAX_MPROTECT;
+
+	return pax_flags;
+}
+KH_FEATURE_END(CONFIG_PAX_PT_PAX_FLAGS)
+
+
+
+static unsigned long pax_parse_defaults(void)
+{
+	unsigned long pax_flags = 0UL;
+
+	pax_flags |= MF_PAX_PAGEEXEC;
+
+	pax_flags |= MF_PAX_SEGMEXEC;
+
+	pax_flags |= MF_PAX_MPROTECT;
+
+	return pax_flags;
+}
+
+static unsigned long pax_parse_ei_pax(const struct elfhdr * const elf_ex)
+{
+	unsigned long pax_flags = PAX_PARSE_FLAGS_FALLBACK;
+
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"Per exec config?")
+	if (!(elf_ex->e_ident[EI_PAX] & EF_PAX_PAGEEXEC))
+		pax_flags |= MF_PAX_PAGEEXEC;
+KH_FEATURE_END(PAX_PAGEEXEC)
+
+KH_FEATURE_BEGIN(PAX_MPROTECT,"Per exec config?")
+	if ((pax_flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) && !(elf_ex->e_ident[EI_PAX] & EF_PAX_MPROTECT))
+		pax_flags |= MF_PAX_MPROTECT;
+KH_FEATURE_END(PAX_MPROTECT)
+
+	return pax_flags;
+}
+
+static unsigned long pax_parse_pt_pax(const struct elfhdr * const elf_ex, const struct elf_phdr * const elf_phdata)
+{
+
+KH_FEATURE_BEGIN(CONFIG_PAX_PT_PAX_FLAGS,"Implement hardmode only")
+	unsigned long i;
+
+	for (i = 0UL; i < elf_ex->e_phnum; i++)
+		if (elf_phdata[i].p_type == PT_PAX_FLAGS) {
+			if (((elf_phdata[i].p_flags & PF_PAGEEXEC) && (elf_phdata[i].p_flags & PF_NOPAGEEXEC)) ||
+			    ((elf_phdata[i].p_flags & PF_SEGMEXEC) && (elf_phdata[i].p_flags & PF_NOSEGMEXEC)) ||
+			    ((elf_phdata[i].p_flags & PF_EMUTRAMP) && (elf_phdata[i].p_flags & PF_NOEMUTRAMP)) ||
+			    ((elf_phdata[i].p_flags & PF_MPROTECT) && (elf_phdata[i].p_flags & PF_NOMPROTECT)) ||
+			    ((elf_phdata[i].p_flags & PF_RANDMMAP) && (elf_phdata[i].p_flags & PF_NORANDMMAP)))
+				return PAX_PARSE_FLAGS_FALLBACK;
+
+				return pax_parse_pt_pax_hardmode(&elf_phdata[i]);
+			break;
+		}
+KH_FEATURE_END(CONFIG_PAX_PT_PAX_FLAGS)
+
+	return PAX_PARSE_FLAGS_FALLBACK;
+}
+
+static unsigned long pax_parse_xattr_pax(struct file * const file)
+{
+
+	return PAX_PARSE_FLAGS_FALLBACK;
+
+}
+
+static long pax_parse_pax_flags(const struct elfhdr * const elf_ex, const struct elf_phdr * const elf_phdata, struct file * const file)
+{
+	unsigned long pax_flags, ei_pax_flags,  pt_pax_flags, xattr_pax_flags;
+
+	pax_flags = pax_parse_defaults();
+	ei_pax_flags = pax_parse_ei_pax(elf_ex);
+	pt_pax_flags = pax_parse_pt_pax(elf_ex, elf_phdata);
+	xattr_pax_flags = pax_parse_xattr_pax(file);
+
+	if (pt_pax_flags != PAX_PARSE_FLAGS_FALLBACK &&
+	    xattr_pax_flags != PAX_PARSE_FLAGS_FALLBACK &&
+	    pt_pax_flags != xattr_pax_flags)
+		return -EINVAL;
+
+	if (xattr_pax_flags != PAX_PARSE_FLAGS_FALLBACK)
+		pax_flags = xattr_pax_flags;
+	else if (pt_pax_flags != PAX_PARSE_FLAGS_FALLBACK)
+		pax_flags = pt_pax_flags;
+	else if (ei_pax_flags != PAX_PARSE_FLAGS_FALLBACK)
+		pax_flags = ei_pax_flags;
+
+	if ((pax_flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) == (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) {
+		if ((__supported_pte_mask & _PAGE_NX))
+			pax_flags &= ~MF_PAX_SEGMEXEC;
+		else
+			pax_flags &= ~MF_PAX_PAGEEXEC;
+	}
+
+	if (0 > pax_check_flags(&pax_flags))
+		return -EINVAL;
+
+	current->mm->pax_flags = pax_flags;
+	return 0;
+}
+
+KH_FEATURE_END(PAX_NOEXEC)
+
+
+
+
 /*
  * These are the functions used to load ELF style executables and shared
  * libraries.  There is no binary dependent code anywhere else.
@@ -591,6 +726,10 @@ static int load_elf_binary(struct linux_binprm *bprm)
 		struct elfhdr interp_elf_ex;
 	} *loc;
 
+	KH_FEATURE_BEGIN(PAX_NOEXEC,"pax_task_size")
+	unsigned long pax_task_size = TASK_SIZE;
+	KH_FEATURE_END(PAX_NOEXEC)
+
 	loc = kmalloc(sizeof(*loc), GFP_KERNEL);
 	if (!loc) {
 		retval = -ENOMEM;
@@ -727,8 +866,43 @@ static int load_elf_binary(struct linux_binprm *bprm)
 	/* Do this immediately, since STACK_TOP as used in setup_arg_pages
 	   may depend on the personality.  */
 	SET_PERSONALITY(loc->elf_ex);
+
+	KH_FEATURE_BEGIN(PAX_NOEXEC,"Initialize pax_flags in mm_struct")
+	current->mm->pax_flags = 0UL;
+	KH_FEATURE_END(PAX_NOEXEC)
+
+	KH_FEATURE_BEGIN(PAX_NOEXEC,"Parse PAX ELF flags")
+	if (0 > pax_parse_pax_flags(&loc->elf_ex, elf_phdata, bprm->file)) {
+		send_sig(SIGKILL, current, 0);
+		goto out_free_dentry;
+	}
+	KH_FEATURE_END(PAX_NOEXEC)
+
+
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"TBI depends on CONFIG_ARCH_TRACK_EXEC_LIMIT may not be PAX_PAGEEXEC")
+#ifdef CONFIG_ARCH_TRACK_EXEC_LIMIT
+	if ((current->mm->pax_flags & MF_PAX_PAGEEXEC) && !(__supported_pte_mask & _PAGE_NX)) {
+		current->mm->context.user_cs_limit = PAGE_SIZE;
+		current->mm->def_flags |= VM_PAGEEXEC | VM_NOHUGEPAGE;
+	}
+#endif
+	KH_FEATURE_END(PAX_PAGEEXEC)
+
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+	KH_FEATURE_BEGIN(PAX_SEGMEXEC,"")
+	if (current->mm->pax_flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) {
+		executable_stack = EXSTACK_DISABLE_X;
+		current->personality &= ~READ_IMPLIES_EXEC;
+	} else
+	KH_FEATURE_END(PAX_SEGMEXEC)
+	KH_FEATURE_END(PAX_PAGEEXEC)
+
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"Linked to previous PAX_PAGEEXEC block. See else statement!!!")
+	KH_FEATURE_BEGIN(PAX_SEGMEXEC,"")
 	if (elf_read_implies_exec(loc->elf_ex, executable_stack))
 		current->personality |= READ_IMPLIES_EXEC;
+	KH_FEATURE_END(PAX_SEGMEXEC)
+	KH_FEATURE_END(PAX_PAGEEXEC)
 
 	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
 		current->flags |= PF_RANDOMIZE;
@@ -2203,6 +2377,149 @@ out:
 
 #endif		/* CONFIG_ELF_CORE */
 
+
+KH_FEATURE_BEGIN(PAX_MPROTECT,"elf_handle_mprotect implementation")
+/* PaX: non-PIC ELF libraries need relocations on their executable segments
+ * therefore we'll grant them VM_MAYWRITE once during their life. Similarly
+ * we'll remove VM_MAYWRITE for good on RELRO segments.
+ *
+ * The checks favour ld-linux.so behaviour which operates on a per ELF segment
+ * basis because we want to allow the common case and not the special ones.
+ */
+static void elf_handle_mprotect(struct vm_area_struct *vma, unsigned long newflags)
+{
+	struct elfhdr elf_h;
+	struct elf_phdr elf_p;
+	unsigned long i;
+	unsigned long oldflags;
+	bool is_textrel_rw, is_textrel_rx, is_relro;
+
+	if (!(vma->vm_mm->pax_flags & MF_PAX_MPROTECT) || !vma->vm_file)
+		return;
+
+	oldflags = vma->vm_flags & (VM_MAYEXEC | VM_MAYWRITE | VM_MAYREAD | VM_EXEC | VM_WRITE | VM_READ);
+	newflags &= VM_MAYEXEC | VM_MAYWRITE | VM_MAYREAD | VM_EXEC | VM_WRITE | VM_READ;
+
+#ifdef CONFIG_PAX_ELFRELOCS
+	/* possible TEXTREL */
+	is_textrel_rw = !vma->anon_vma && oldflags == (VM_MAYEXEC | VM_MAYREAD | VM_EXEC | VM_READ) && newflags == (VM_WRITE | VM_READ);
+	is_textrel_rx = vma->anon_vma && oldflags == (VM_MAYEXEC | VM_MAYWRITE | VM_MAYREAD | VM_WRITE | VM_READ) && newflags == (VM_EXEC | VM_READ);
+#else
+	is_textrel_rw = false;
+	is_textrel_rx = false;
+#endif
+
+	/* possible RELRO */
+	is_relro = vma->anon_vma && oldflags == (VM_MAYWRITE | VM_MAYREAD | VM_READ) && newflags == (VM_MAYWRITE | VM_MAYREAD | VM_READ);
+
+	if (!is_textrel_rw && !is_textrel_rx && !is_relro)
+		return;
+
+	if (sizeof(elf_h) != kernel_read(vma->vm_file, 0UL, (char *)&elf_h, sizeof(elf_h)) ||
+	    memcmp(elf_h.e_ident, ELFMAG, SELFMAG) ||
+
+#ifdef CONFIG_PAX_ETEXECRELOCS
+	    ((is_textrel_rw || is_textrel_rx) && (elf_h.e_type != ET_DYN && elf_h.e_type != ET_EXEC)) ||
+#else
+	    ((is_textrel_rw || is_textrel_rx) && elf_h.e_type != ET_DYN) ||
+#endif
+
+	    (is_relro && (elf_h.e_type != ET_DYN && elf_h.e_type != ET_EXEC)) ||
+	    !elf_check_arch(&elf_h) ||
+	    elf_h.e_phentsize != sizeof(struct elf_phdr) ||
+	    elf_h.e_phnum > 65536UL / sizeof(struct elf_phdr))
+		return;
+
+	for (i = 0UL; i < elf_h.e_phnum; i++) {
+		if (sizeof(elf_p) != kernel_read(vma->vm_file, elf_h.e_phoff + i*sizeof(elf_p), (char *)&elf_p, sizeof(elf_p)))
+			return;
+		switch (elf_p.p_type) {
+		case PT_DYNAMIC:
+			if (!is_textrel_rw && !is_textrel_rx)
+				continue;
+
+/* >>GRSEC PAX_MPROTECT elf_dyn not defined TBI */
+#if 0			
+			i = 0UL;
+			while ((i+1) * sizeof(elf_dyn) <= elf_p.p_filesz) {
+				elf_dyn dyn;
+
+				if (sizeof(dyn) != kernel_read(vma->vm_file, elf_p.p_offset + i*sizeof(dyn), (char *)&dyn, sizeof(dyn)))
+					break;
+				if (dyn.d_tag == DT_NULL)
+					break;
+				if (dyn.d_tag == DT_TEXTREL || (dyn.d_tag == DT_FLAGS && (dyn.d_un.d_val & DF_TEXTREL))) {
+					gr_log_textrel(vma);
+					if (is_textrel_rw)
+						vma->vm_flags |= VM_MAYWRITE;
+					else
+						/* PaX: disallow write access after relocs are done, hopefully noone else needs it... */
+						vma->vm_flags &= ~VM_MAYWRITE;
+					break;
+				}
+				i++;
+			}
+#endif			
+/* <<GRSEC PAX_MPROTECT */
+			is_textrel_rw = false;
+			is_textrel_rx = false;
+			continue;
+
+/* >>GRSEC PAX_MPROTECT PT_GNU_RELRO not defined TBI */
+#if 0			
+		case PT_GNU_RELRO:
+			if (!is_relro)
+				continue;
+			if ((elf_p.p_offset >> PAGE_SHIFT) == vma->vm_pgoff && ELF_PAGEALIGN(elf_p.p_memsz) == vma->vm_end - vma->vm_start)
+				vma->vm_flags &= ~VM_MAYWRITE;
+			is_relro = false;
+			continue;
+#endif		
+/* <<GRSEC PAX_MPROTECT */
+
+#ifdef CONFIG_PAX_PT_PAX_FLAGS
+		case PT_PAX_FLAGS: {
+			const char *msg_mprotect = "", *msg_emutramp = "";
+			char *buffer_lib, *buffer_exe;
+
+			if (elf_p.p_flags & PF_NOMPROTECT)
+				msg_mprotect = "MPROTECT disabled";
+
+#ifdef CONFIG_PAX_EMUTRAMP
+			if (!(vma->vm_mm->pax_flags & MF_PAX_EMUTRAMP) && !(elf_p.p_flags & PF_NOEMUTRAMP))
+				msg_emutramp = "EMUTRAMP enabled";
+#endif
+
+			if (!msg_mprotect[0] && !msg_emutramp[0])
+				continue;
+
+			if (!printk_ratelimit())
+				continue;
+
+			buffer_lib = (char *)__get_free_page(GFP_KERNEL);
+			buffer_exe = (char *)__get_free_page(GFP_KERNEL);
+			if (buffer_lib && buffer_exe) {
+				char *path_lib, *path_exe;
+
+				path_lib = pax_get_path(&vma->vm_file->f_path, buffer_lib, PAGE_SIZE);
+				path_exe = pax_get_path(&vma->vm_mm->exe_file->f_path, buffer_exe, PAGE_SIZE);
+
+				pr_info("PAX: %s wants %s%s%s on %s\n", path_lib, msg_mprotect,
+					(msg_mprotect[0] && msg_emutramp[0] ? " and " : ""), msg_emutramp, path_exe);
+
+			}
+			free_page((unsigned long)buffer_exe);
+			free_page((unsigned long)buffer_lib);
+			continue;
+		}
+#endif
+
+		}
+	}
+}
+KH_FEATURE_END(PAX_MPROTECT)
+
+
 static int __init init_elf_binfmt(void)
 {
 	register_binfmt(&elf_format);
diff --git a/fs/exec.c b/fs/exec.c
index b7a5f46..933e34f 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -56,6 +56,7 @@
 #include <linux/pipe_fs_i.h>
 #include <linux/oom.h>
 #include <linux/compat.h>
+#include <linux/coredump.h>
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -66,6 +67,8 @@
 
 #include <trace/events/sched.h>
 
+#include <kh/kh_common.h>
+
 int suid_dumpable = 0;
 
 static LIST_HEAD(formats);
@@ -716,6 +719,21 @@ int setup_arg_pages(struct linux_binprm *bprm,
 			goto out_unlock;
 	}
 
+	KH_FEATURE_BEGIN(PAX_PAGEEXEC,"Beware here, Contextual code very different TBI")
+	KH_FEATURE_BEGIN(PAX_SEGMEXEC,"Imbricated")
+	if (mm->pax_flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) {
+		vm_flags &= ~VM_EXEC;
+
+	KH_FEATURE_BEGIN(PAX_MPROTECT,"Imbricated")
+		if (mm->pax_flags & MF_PAX_MPROTECT)
+			vm_flags &= ~VM_MAYEXEC;
+	KH_FEATURE_END(PAX_MPROTECT)
+
+	}
+	KH_FEATURE_END(PAX_SEGMEXEC)
+	KH_FEATURE_END(PAX_PAGEEXEC)
+
+
 	/* mprotect_fixup is overkill to remove the temporary stack flags */
 	vma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;
 
@@ -1639,3 +1657,156 @@ COMPAT_SYSCALL_DEFINE3(execve, const char __user *, filename,
 	return compat_do_execve(getname(filename), argv, envp);
 }
 #endif
+
+KH_FEATURE_BEGIN(PAX_NOEXEC,"pax_check_flags function implementation")
+int pax_check_flags(unsigned long *flags)
+{
+	int retval = 0;
+
+
+/* GRSEC_TBD Deactivate for now */
+#if 0
+#if !defined(CONFIG_X86_32) || !defined(CONFIG_PAX_SEGMEXEC)
+	if (*flags & MF_PAX_SEGMEXEC)
+	{
+		*flags &= ~MF_PAX_SEGMEXEC;
+
+	retval = -EINVAL;
+	}
+#endif
+#endif
+
+	if ((*flags & MF_PAX_PAGEEXEC)
+
+/* GRSEC_TBI Does not understand this ? */
+/*#ifdef CONFIG_PAX_PAGEEXEC*/
+	    &&  (*flags & MF_PAX_SEGMEXEC)
+/*#endif*/
+
+	   )
+	{
+		*flags &= ~MF_PAX_PAGEEXEC;
+		retval = -EINVAL;
+	}
+
+/* GRSEC_TBD Deactivate for now */
+	if ((*flags & MF_PAX_MPROTECT)
+
+	    && !(*flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC))
+
+	   )
+	{
+		*flags &= ~MF_PAX_MPROTECT;
+		retval = -EINVAL;
+	}
+
+	if ((*flags & MF_PAX_EMUTRAMP)
+
+#ifdef CONFIG_PAX_EMUTRAMP
+	    && !(*flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC))
+#endif
+
+	   )
+	{
+		*flags &= ~MF_PAX_EMUTRAMP;
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+EXPORT_SYMBOL(pax_check_flags);
+
+KH_FEATURE_END(PAX_NOEXEC)
+
+
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"pax_get_path function implementation")
+
+char *pax_get_path(const struct path *path, char *buf, int buflen)
+{
+	char *pathname = d_path(path, buf, buflen);
+
+	if (IS_ERR(pathname))
+		goto toolong;
+
+	pathname = mangle_path(buf, pathname, "\t\n\\");
+	if (!pathname)
+		goto toolong;
+
+	*pathname = 0;
+	return buf;
+
+toolong:
+	return "<path too long>";
+}
+EXPORT_SYMBOL(pax_get_path);
+
+KH_FEATURE_END(PAX_PAGEEXEC)
+
+
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"pax_report_fault function implementation")
+
+void pax_report_fault(struct pt_regs *regs, void *pc, void *sp)
+{
+	struct task_struct *tsk = current;
+	struct mm_struct *mm = current->mm;
+	char *buffer_exec = (char *)__get_free_page(GFP_KERNEL);
+	char *buffer_fault = (char *)__get_free_page(GFP_KERNEL);
+	char *path_exec = NULL;
+	char *path_fault = NULL;
+	unsigned long start = 0UL, end = 0UL, offset = 0UL;
+	siginfo_t info = { };
+
+	if (buffer_exec && buffer_fault) {
+		struct vm_area_struct *vma, *vma_exec = NULL, *vma_fault = NULL;
+
+		down_read(&mm->mmap_sem);
+		vma = mm->mmap;
+		while (vma && (!vma_exec || !vma_fault)) {
+			if (vma->vm_file && mm->exe_file == vma->vm_file && (vma->vm_flags & VM_EXEC))
+				vma_exec = vma;
+			if (vma->vm_start <= (unsigned long)pc && (unsigned long)pc < vma->vm_end)
+				vma_fault = vma;
+			vma = vma->vm_next;
+		}
+		if (vma_exec)
+			path_exec = pax_get_path(&vma_exec->vm_file->f_path, buffer_exec, PAGE_SIZE);
+		if (vma_fault) {
+			start = vma_fault->vm_start;
+			end = vma_fault->vm_end;
+			offset = vma_fault->vm_pgoff << PAGE_SHIFT;
+			if (vma_fault->vm_file)
+				path_fault = pax_get_path(&vma_fault->vm_file->f_path, buffer_fault, PAGE_SIZE);
+			else if ((unsigned long)pc >= mm->start_brk && (unsigned long)pc < mm->brk)
+				path_fault = "<heap>";
+			else if (vma_fault->vm_flags & (VM_GROWSDOWN | VM_GROWSUP))
+				path_fault = "<stack>";
+			else
+				path_fault = "<anonymous mapping>";
+		}
+		up_read(&mm->mmap_sem);
+	}
+
+
+	/* >>GRSEC TBI curr_ip depends on CONFIG_GRKERNSEC which is not enabled in this context */
+#if 0
+	if (tsk->signal->curr_ip)
+		printk(KERN_ERR "PAX: From %pI4: execution attempt in: %s, %08lx-%08lx %08lx\n", &tsk->signal->curr_ip, path_fault, start, end, offset);
+	else
+#endif
+		printk(KERN_ERR "PAX: execution attempt in: %s, %08lx-%08lx %08lx\n", path_fault, start, end, offset);
+	printk(KERN_ERR "PAX: terminating task: %s(%s):%d, uid/euid: %u/%u, PC: %p, SP: %p\n", path_exec, tsk->comm, task_pid_nr(tsk),
+			from_kuid_munged(&init_user_ns, task_uid(tsk)), from_kuid_munged(&init_user_ns, task_euid(tsk)), pc, sp);
+	free_page((unsigned long)buffer_exec);
+	free_page((unsigned long)buffer_fault);
+	pax_report_insns(regs, pc, sp);
+	info.si_signo = SIGKILL;
+	info.si_errno = 0;
+	info.si_code = SI_KERNEL;
+	info.si_pid = 0;
+	info.si_uid = 0;
+	do_coredump(&info);
+}
+
+KH_FEATURE_END(PAX_PAGEEXEC)
+
diff --git a/fs/proc/array.c b/fs/proc/array.c
index cd3653e..50c1d8d 100644
--- a/fs/proc/array.c
+++ b/fs/proc/array.c
@@ -87,6 +87,8 @@
 #include <asm/processor.h>
 #include "internal.h"
 
+#include <kh/kh_common.h>
+
 static inline void task_name(struct seq_file *m, struct task_struct *p)
 {
 	int i;
@@ -347,6 +349,21 @@ static void task_cpus_allowed(struct seq_file *m, struct task_struct *task)
 	seq_putc(m, '\n');
 }
 
+KH_FEATURE_BEGIN(PAX_NOEXEC,"task_pax implementation")
+static inline void task_pax(struct seq_file *m, struct task_struct *p)
+{
+	if (p->mm)
+		seq_printf(m, "PaX:\t%c%c%c%c%c\n",
+			   p->mm->pax_flags & MF_PAX_PAGEEXEC ? 'P' : 'p',
+			   p->mm->pax_flags & MF_PAX_EMUTRAMP ? 'E' : 'e',
+			   p->mm->pax_flags & MF_PAX_MPROTECT ? 'M' : 'm',
+			   p->mm->pax_flags & MF_PAX_RANDMMAP ? 'R' : 'r',
+			   p->mm->pax_flags & MF_PAX_SEGMEXEC ? 'S' : 's');
+	else
+		seq_printf(m, "PaX:\t-----\n");
+}
+KH_FEATURE_END(PAX_NOEXEC)
+
 int proc_pid_status(struct seq_file *m, struct pid_namespace *ns,
 			struct pid *pid, struct task_struct *task)
 {
@@ -365,6 +382,12 @@ int proc_pid_status(struct seq_file *m, struct pid_namespace *ns,
 	task_cpus_allowed(m, task);
 	cpuset_task_status_allowed(m, task);
 	task_context_switch_counts(m, task);
+
+	KH_FEATURE_BEGIN(PAX_NOEXEC,"task_pax invocation")
+	task_pax(m, task);
+	KH_FEATURE_END(PAX_NOEXEC)
+
+
 	return 0;
 }
 
diff --git a/grsecurity/Kconfig b/grsecurity/Kconfig
new file mode 100644
index 0000000..6188896
--- /dev/null
+++ b/grsecurity/Kconfig
@@ -0,0 +1,5 @@
+config PAX_MPROTECT_NOERR
+	bool "Disable errors on PAX mprotect exceptions"
+	depends on MODULES
+	help
+	  Disable errors on PAX mprotect exceptions
diff --git a/include/kh/kh_common.h b/include/kh/kh_common.h
new file mode 100644
index 0000000..deef992
--- /dev/null
+++ b/include/kh/kh_common.h
@@ -0,0 +1,32 @@
+#ifndef __KH_COMMON_H__
+#define __KH_COMMON_H__
+
+/*
+ * GRSecurity related macros
+ *
+ */
+
+
+#define PAX_NOEXEC   (0x10000000)
+#define PAX_PAGEEXEC ((PAX_NOEXEC) | (0x01))
+#define PAX_SEGMEXEC ((PAX_NOEXEC) | (0x02))
+#define PAX_MPROTECT ((PAX_NOEXEC) | (0x04))
+
+#define KH_FEATURE_BEGIN(ID,COMMENT)
+#define KH_FEATURE_END(ID)
+
+/*
+ * Temporary
+ *
+ */
+
+/* GRSEC_TBD __supported_pte_mask & _PAGE_NX implementation */
+#ifndef __supported_pte_mask
+#define __supported_pte_mask (0)
+#endif
+
+#ifndef _PAGE_NX
+#define _PAGE_NX (0)
+#endif
+
+#endif /*__KH_COMMON_H__*/
diff --git a/include/linux/binfmts.h b/include/linux/binfmts.h
index 61f29e5..f422a6f 100644
--- a/include/linux/binfmts.h
+++ b/include/linux/binfmts.h
@@ -73,6 +73,10 @@ struct linux_binfmt {
 	int (*load_binary)(struct linux_binprm *);
 	int (*load_shlib)(struct file *);
 	int (*core_dump)(struct coredump_params *cprm);
+/* >>GRSEC PAX_MPROTECT */
+	void (*handle_mprotect)(struct vm_area_struct *vma, unsigned long newflags);
+	void (*handle_mmap)(struct file *);	
+/* <<GRSEC PAX_MPROTECT */
 	unsigned long min_coredump;	/* minimal dump size */
 };
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 86a977b..d39e456 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -128,6 +128,14 @@ extern unsigned int kobjsize(const void *objp);
 #define VM_HUGETLB	0x00400000	/* Huge TLB Page VM */
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_ARCH_1	0x01000000	/* Architecture-specific flag */
+
+
+/* >>GRSEC PAX_PAGEEXEC */
+#if defined(CONFIG_X86_32)
+#define VM_PAGEEXEC	0x02000000	/* vma->vm_page_prot needs special handling */
+#endif
+/* <<GRSEC PAX_PAGEEXEC */
+
 #define VM_DONTDUMP	0x04000000	/* Do not include in the core dump */
 
 #ifdef CONFIG_MEM_SOFT_DIRTY
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 6e0b286..387be21 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -454,6 +454,10 @@ struct mm_struct {
 	bool tlb_flush_pending;
 #endif
 	struct uprobes_state uprobes_state;
+
+	/* >>GRSEC PAX_NOEXEC */
+	unsigned long pax_flags;	
+	/* <<GRSEC PAX_NOEXEC */
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e344bb..575d10a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1663,6 +1663,48 @@ struct task_struct {
 #endif
 };
 
+/* >>GRSEC PAX_GENERIC TBI */
+#define MF_PAX_PAGEEXEC		0x01000000	/* Paging based non-executable pages */
+#define MF_PAX_EMUTRAMP		0x02000000	/* Emulate trampolines */
+#define MF_PAX_MPROTECT		0x04000000	/* Restrict mprotect() */
+#define MF_PAX_RANDMMAP		0x08000000	/* Randomize mmap() base */
+/*#define MF_PAX_RANDEXEC		0x10000000*/	/* Randomize ET_EXEC base */
+#define MF_PAX_SEGMEXEC		0x20000000	/* Segmentation based non-executable pages */
+
+extern int pax_check_flags(unsigned long *);
+#define PAX_PARSE_FLAGS_FALLBACK	(~0UL)
+/* <<GRSEC PAX_GENERIC TBI */
+
+/* >>GRSEC PAX_NOEXEC */
+/* if tsk != current then task_lock must be held on it */
+static inline unsigned long pax_get_flags(struct task_struct *tsk)
+{
+	if (likely(tsk->mm))
+		return tsk->mm->pax_flags;
+	else
+		return 0UL;
+}
+
+/* if tsk != current then task_lock must be held on it */
+static inline long pax_set_flags(struct task_struct *tsk, unsigned long flags)
+{
+	if (likely(tsk->mm)) {
+		tsk->mm->pax_flags = flags;
+		return 0;
+	}
+	return -EINVAL;
+}
+/* <<GRSEC PAX_NOEXEC */
+
+/* >>GRSEC PAX_GENERIC TBI */
+struct path;
+extern char *pax_get_path(const struct path *path, char *buf, int buflen);
+extern void pax_report_fault(struct pt_regs *regs, void *pc, void *sp);
+extern void pax_report_insns(struct pt_regs *regs, void *pc, void *sp);
+extern void pax_report_refcount_overflow(struct pt_regs *regs);
+/* <<GRSEC PAX_GENERIC TBI */
+
+
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
diff --git a/include/uapi/linux/a.out.h b/include/uapi/linux/a.out.h
index 7caf44c..a8035e8f 100644
--- a/include/uapi/linux/a.out.h
+++ b/include/uapi/linux/a.out.h
@@ -39,6 +39,16 @@ enum machine_type {
   M_MIPS2 = 152		/* MIPS R6000/R4000 binary */
 };
 
+/* >>GRSEC PAX_PAGEEXEC */
+/* Constants for the N_FLAGS field */
+#define F_PAX_PAGEEXEC  1 /* Paging based non-executable pages */
+#define F_PAX_EMUTRAMP  2 /* Emulate trampolines */
+#define F_PAX_MPROTECT  4 /* Restrict mprotect() */
+#define F_PAX_RANDMMAP  8 /* Randomize mmap() base */
+/*#define F_PAX_RANDEXEC  16*/  /* Randomize ET_EXEC base */
+#define F_PAX_SEGMEXEC  32  /* Segmentation based non-executable pages */
+/* <<GRSEC PAX_PAGEEXEC */
+
 #if !defined (N_MAGIC)
 #define N_MAGIC(exec) ((exec).a_info & 0xffff)
 #endif
diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
index ea9bf25..d8bdbf6 100644
--- a/include/uapi/linux/elf.h
+++ b/include/uapi/linux/elf.h
@@ -38,6 +38,18 @@ typedef __s64	Elf64_Sxword;
 
 #define PT_GNU_STACK	(PT_LOOS + 0x474e551)
 
+/* >>GRSEC PAX_PAGEEXEC */
+#define PT_PAX_FLAGS  (PT_LOOS + 0x5041580)
+
+/* Constants for the e_flags field */
+#define EF_PAX_PAGEEXEC   1 /* Paging based non-executable pages */
+#define EF_PAX_EMUTRAMP   2 /* Emulate trampolines */
+#define EF_PAX_MPROTECT   4 /* Restrict mprotect() */
+#define EF_PAX_RANDMMAP   8 /* Randomize mmap() base */
+/*#define EF_PAX_RANDEXEC   16*/  /* Randomize ET_EXEC base */
+#define EF_PAX_SEGMEXEC   32  /* Segmentation based non-executable pages */
+/* <<GRSEC PAX_PAGEEXEC */
+
 /*
  * Extended Numbering
  *
@@ -240,6 +252,22 @@ typedef struct elf64_hdr {
 #define PF_W		0x2
 #define PF_X		0x1
 
+/* >>GRSEC CONFIG_PAX_PT_PAX_FLAGS */
+#define PF_PAGEEXEC (1U << 4) /* Enable  PAGEEXEC */
+#define PF_NOPAGEEXEC (1U << 5) /* Disable PAGEEXEC */
+#define PF_SEGMEXEC (1U << 6) /* Enable  SEGMEXEC */
+#define PF_NOSEGMEXEC (1U << 7) /* Disable SEGMEXEC */
+#define PF_MPROTECT (1U << 8) /* Enable  MPROTECT */
+#define PF_NOMPROTECT (1U << 9) /* Disable MPROTECT */
+/*#define PF_RANDEXEC (1U << 10)*/  /* Enable  RANDEXEC */
+/*#define PF_NORANDEXEC (1U << 11)*/  /* Disable RANDEXEC */
+#define PF_EMUTRAMP (1U << 12)  /* Enable  EMUTRAMP */
+#define PF_NOEMUTRAMP (1U << 13)  /* Disable EMUTRAMP */
+#define PF_RANDMMAP (1U << 14)  /* Enable  RANDMMAP */
+#define PF_NORANDMMAP (1U << 15)  /* Disable RANDMMAP */
+/* <<GRSEC CONFIG_PAX_PT_PAX_FLAGS */
+
+
 typedef struct elf32_phdr{
   Elf32_Word	p_type;
   Elf32_Off	p_offset;
@@ -332,6 +360,10 @@ typedef struct elf64_shdr {
 #define	EI_OSABI	7
 #define	EI_PAD		8
 
+/* >>GRSEC PAX_PAGEEXEC */
+#define EI_PAX    14
+/* <<GRSEC PAX_PAGEEXEC */
+
 #define	ELFMAG0		0x7f		/* EI_MAG */
 #define	ELFMAG1		'E'
 #define	ELFMAG2		'L'
diff --git a/ipc/shm.c b/ipc/shm.c
index 0145479..aef1e9e 100644
--- a/ipc/shm.c
+++ b/ipc/shm.c
@@ -47,6 +47,9 @@
 
 #include "util.h"
 
+#include <kh/kh_common.h>
+
+
 struct shm_file_data {
 	int id;
 	struct ipc_namespace *ns;
@@ -1095,6 +1098,12 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,
 		f_mode = FMODE_READ | FMODE_WRITE;
 	}
 	if (shmflg & SHM_EXEC) {
+
+		KH_FEATURE_BEGIN(PAX_MPROTECT,"")
+		if (current->mm->pax_flags & MF_PAX_MPROTECT)
+			goto out;
+		KH_FEATURE_END(PAX_MPROTECT)
+
 		prot |= PROT_EXEC;
 		acc_mode |= S_IXUGO;
 	}
diff --git a/mm/mmap.c b/mm/mmap.c
index f88b4f9..462a8de 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -49,6 +49,8 @@
 
 #include "internal.h"
 
+#include <kh/kh_common.h>
+
 #ifndef arch_mmap_check
 #define arch_mmap_check(addr, len, flags)	(0)
 #endif
@@ -83,9 +85,20 @@ pgprot_t protection_map[16] = {
 
 pgprot_t vm_get_page_prot(unsigned long vm_flags)
 {
-	return __pgprot(pgprot_val(protection_map[vm_flags &
+	pgprot_t prot = __pgprot(pgprot_val(protection_map[vm_flags &
 				(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |
 			pgprot_val(arch_vm_get_page_prot(vm_flags)));
+
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+#if defined(CONFIG_X86_32)
+	if (!(__supported_pte_mask & _PAGE_NX) &&
+	    (vm_flags & (VM_PAGEEXEC | VM_EXEC)) == VM_PAGEEXEC &&
+	    (vm_flags & (VM_READ | VM_WRITE)))
+		prot = __pgprot(pte_val(pte_exprotect(__pte(pgprot_val(prot)))));
+#endif
+KH_FEATURE_END(PAX_PAGEEXEC)
+
+	return prot;
 }
 EXPORT_SYMBOL(vm_get_page_prot);
 
@@ -1310,6 +1323,50 @@ unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,
 	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
 			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 
+
+KH_FEATURE_BEGIN(PAX_MPROTECT,"")
+	if (mm->pax_flags & MF_PAX_MPROTECT) {
+
+#ifdef CONFIG_GRKERNSEC_RWXMAP_LOG
+		if (file && !pgoff && (vm_flags & VM_EXEC) && mm->binfmt &&
+		    mm->binfmt->handle_mmap)
+			mm->binfmt->handle_mmap(file);
+#endif
+
+#ifndef CONFIG_PAX_MPROTECT_COMPAT
+		if ((vm_flags & (VM_WRITE | VM_EXEC)) == (VM_WRITE | VM_EXEC)) {
+			/* >>GRSEC Audit specific: disable */
+#if 0
+			gr_log_rwxmmap(file);
+#endif
+			/* <<GRSEC */
+
+#ifdef CONFIG_PAX_EMUPLT
+			vm_flags &= ~VM_EXEC;
+#else
+			return -EPERM;
+#endif
+
+		}
+
+		if (!(vm_flags & VM_EXEC))
+			vm_flags &= ~VM_MAYEXEC;
+#else
+		if ((vm_flags & (VM_WRITE | VM_EXEC)) != VM_EXEC)
+			vm_flags &= ~(VM_EXEC | VM_MAYEXEC);
+#endif
+		else
+			vm_flags &= ~VM_MAYWRITE;
+	}
+KH_FEATURE_END(PAX_MPROTECT)
+
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+#if defined(CONFIG_X86_32)
+	if ((mm->pax_flags & MF_PAX_PAGEEXEC) && file)
+		vm_flags &= ~VM_PAGEEXEC;
+#endif
+KH_FEATURE_END(PAX_PAGEEXEC)
+
 	if (flags & MAP_LOCKED)
 		if (!can_do_mlock())
 			return -EPERM;
@@ -1625,6 +1682,16 @@ munmap_back:
 		if (error)
 			goto unmap_and_free_vma;
 
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+#if defined(CONFIG_X86_32)
+		if ((mm->pax_flags & MF_PAX_PAGEEXEC) && !(vma->vm_flags & VM_SPECIAL)) {
+			vma->vm_flags |= VM_PAGEEXEC;
+			vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
+		}
+#endif
+KH_FEATURE_END(PAX_PAGEEXEC)
+
+
 		/* Can addr have changed??
 		 *
 		 * Answer: Yes, several device drivers can do it in their
@@ -2658,6 +2725,20 @@ static unsigned long do_brk(unsigned long addr, unsigned long len)
 
 	flags = VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;
 
+KH_FEATURE_BEGIN(PAX_PAGEEXEC,"")
+KH_FEATURE_BEGIN(PAX_SEGMEXEC,"Imbricated")
+	if (mm->pax_flags & (MF_PAX_PAGEEXEC | MF_PAX_SEGMEXEC)) {
+		flags &= ~VM_EXEC;
+
+KH_FEATURE_BEGIN(PAX_MPROTECT,"Imbricated/disabled")
+		if (mm->pax_flags & MF_PAX_MPROTECT)
+			flags &= ~VM_MAYEXEC;
+KH_FEATURE_END(PAX_MPROTECT)
+
+	}
+KH_FEATURE_END(PAX_SEGMEXEC)
+KH_FEATURE_END(PAX_PAGEEXEC)
+
 	error = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);
 	if (error & ~PAGE_MASK)
 		return error;
@@ -2998,6 +3079,22 @@ static struct vm_area_struct *__install_special_mapping(
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
 
+#ifdef CONFIG_PAX_MPROTECT
+	if (mm->pax_flags & MF_PAX_MPROTECT) {
+#ifndef CONFIG_PAX_MPROTECT_COMPAT
+		if ((vm_flags & (VM_WRITE | VM_EXEC)) == (VM_WRITE | VM_EXEC))
+			return -EPERM;
+		if (!(vm_flags & VM_EXEC))
+			vm_flags &= ~VM_MAYEXEC;
+#else
+		if ((vm_flags & (VM_WRITE | VM_EXEC)) != VM_EXEC)
+			vm_flags &= ~(VM_EXEC | VM_MAYEXEC);
+#endif
+		else
+			vm_flags &= ~VM_MAYWRITE;
+	}
+#endif
+
 	vma->vm_flags = vm_flags | mm->def_flags | VM_DONTEXPAND | VM_SOFTDIRTY;
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 
diff --git a/mm/mprotect.c b/mm/mprotect.c
index ace9345..a04f3fc 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -24,11 +24,19 @@
 #include <linux/migrate.h>
 #include <linux/perf_event.h>
 #include <linux/ksm.h>
+
+/* >>GRSEC PAX_MPROTECT Extra header files */ 
+#include <linux/elf.h>
+#include <linux/binfmts.h>
+/* <<GRSEC PAX_MPROTECT */ 
+
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 
+#include <kh/kh_common.h>
+
 /*
  * For a prot_numa update we only hold mmap_sem for read so there is a
  * potential race with faulting where a pmd was temporarily none. This
@@ -315,9 +323,13 @@ success:
 	 * held in write mode.
 	 */
 	vma->vm_flags = newflags;
+	KH_FEATURE_BEGIN(PAX_MPROTECT,"")
+	if (mm->binfmt && mm->binfmt->handle_mprotect)
+		mm->binfmt->handle_mprotect(vma, newflags);
+	KH_FEATURE_END(PAX_MPROTECT)
+
 	dirty_accountable = vma_wants_writenotify(vma);
 	vma_set_page_prot(vma);
-
 	change_protection(vma, start, end, vma->vm_page_prot,
 			  dirty_accountable, 0);
 
@@ -389,6 +401,11 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 	if (start > vma->vm_start)
 		prev = vma;
 
+KH_FEATURE_BEGIN(PAX_MPROTECT,"")
+	if (current->mm->binfmt && current->mm->binfmt->handle_mprotect)
+		current->mm->binfmt->handle_mprotect(vma, vm_flags);
+KH_FEATURE_END(PAX_MPROTECT)
+
 	for (nstart = start ; ; ) {
 		unsigned long newflags;
 
@@ -399,8 +416,12 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 
 		/* newflags >> 4 shift VM_MAY% in place of VM_% */
 		if ((newflags & ~(newflags >> 4)) & (VM_READ | VM_WRITE | VM_EXEC)) {
+			#ifdef CONFIG_PAX_MPROTECT_NOERR
+			printk(KERN_DEBUG "grsecurity mprotect exception (ignored): %08lX\n", newflags);
+			#else
 			error = -EACCES;
 			goto out;
+			#endif
 		}
 
 		error = security_file_mprotect(vma, reqprot, prot);
diff --git a/security/Kconfig b/security/Kconfig
index beb86b5..3815ecb 100644
--- a/security/Kconfig
+++ b/security/Kconfig
@@ -4,6 +4,7 @@
 
 menu "Security options"
 
+source grsecurity/Kconfig
 source security/keys/Kconfig
 
 config SECURITY_DMESG_RESTRICT
-- 
2.4.0.rc1

